

---

# 📊 Dimensionality Reduction Using PCA in ECG Arrhythmia Classification

Principal Component Analysis (PCA) is a widely-used dimensionality reduction technique in machine learning. In this ECG arrhythmia classification project, PCA was applied to reduce the feature space while retaining key information—boosting efficiency and enabling insightful visualizations. Our aim was to preserve **95% of the dataset's variance** using the minimum number of components.

---

## 🔍 Why PCA?

ECG signals, especially when feature-extracted (e.g., RR intervals, waveform morphology), produce high-dimensional data. However, many of these features are correlated or redundant. PCA transforms the dataset into a smaller set of **orthogonal (uncorrelated)** components that capture the most important trends in the data—greatly simplifying model training and visualization.

---

## 🧮 PCA Workflow

### 🔧 Step 1: Performing PCA

```matlab
[coeff, scoreTrain, ~, ~, explained, mu] = pca(X_train);
```

This decomposes the training data into:

* `coeff`: the principal component directions (eigenvectors),
* `scoreTrain`: the projected data (new representation),
* `explained`: variance captured by each component.

> ⚠️ *A warning indicated that some columns were linearly dependent. MATLAB automatically selected only the necessary independent components.*

---

### 🔎 Step 2: Selecting Components (95% Variance)

```matlab
numPCs = find(cumsum(explained) >= 95, 1);
```

📌 **Result**: Only **4 principal components** were needed to preserve over 95% of the total variance—meaning we successfully reduced the dataset from many dimensions down to just 4 with minimal information loss.

---

## 📉 Graphs and What They Show

### 📊 1. **Pareto Chart – Variance Explained**

```matlab
pareto(explained);
xlabel('Principal Component');
ylabel('Variance Explained (%)');
title('PCA Pareto Chart');
```

#### ✅ What It Shows:

* Each bar represents how much variance a component captures.
* The line shows **cumulative variance**.
* **Interpretation**: The first few components contribute the most to explaining the dataset. This helps decide how many components to keep—here, 4 was sufficient to cross 95%.

---

### 🔁 2. **2D PCA Scatter Plots (PC1 vs PC2, PC1 vs PC3, etc.)**

```matlab
for i = 1:numToPlot
    for j = i+1:numToPlot
        gscatter(scoreTrain(:,i), scoreTrain(:,j), Y_train);
        ...
    end
end
```

#### ✅ What They Show:

* Each point is a sample projected in 2D PCA space.
* Points are colored by class (e.g., Normal, SVEB, VEB).
* **Interpretation**: Helps visually assess how well different heartbeat classes are separated in low-dimensional space. Overlaps suggest harder classification boundaries.

📌 **Useful for** identifying class clusters, overlap, and potential misclassifications.

---

### 🧭 3. **3D PCA Scatter Plot**

```matlab
scatter3(scoreTrain(idx,1), scoreTrain(idx,2), scoreTrain(idx,3), ...);
```

#### ✅ What It Shows:

* A 3D view using PC1, PC2, and PC3.
* Each point is colored by class.
* **Interpretation**: Gives a richer spatial understanding of how different arrhythmia classes cluster. If classes form distinct blobs in 3D, models will likely perform well.

> Tip: Use `view(135,30)` to rotate the graph for optimal perspective.

---

## ✅ Final Thoughts

By reducing our feature space from dozens of features to just **4 principal components**, we not only sped up the classification model but also created clear visualizations to understand class separability.

---

### 🧠 Best Practices

* Always apply PCA only on the training data and **transform** the test/validation data using the same `coeff` and `mu`.
* PCA is most effective when followed by model training like Random Forest, k-NN, or SVM for better performance and reduced overfitting.
