

---

# ðŸ“Š Dimensionality Reduction Using PCA in ECG Arrhythmia Classification

Principal Component Analysis (PCA) is a widely-used dimensionality reduction technique in machine learning. In this ECG arrhythmia classification project, PCA was applied to reduce the feature space while retaining key informationâ€”boosting efficiency and enabling insightful visualizations. Our aim was to preserve **95% of the dataset's variance** using the minimum number of components.

---

## ðŸ” Why PCA?

ECG signals, especially when feature-extracted (e.g., RR intervals, waveform morphology), produce high-dimensional data. However, many of these features are correlated or redundant. PCA transforms the dataset into a smaller set of **orthogonal (uncorrelated)** components that capture the most important trends in the dataâ€”greatly simplifying model training and visualization.

---

## ðŸ§® PCA Workflow

### ðŸ”§ Step 1: Performing PCA

```matlab
[coeff, scoreTrain, ~, ~, explained, mu] = pca(X_train);
```

This decomposes the training data into:

* `coeff`: the principal component directions (eigenvectors),
* `scoreTrain`: the projected data (new representation),
* `explained`: variance captured by each component.

> âš ï¸ *A warning indicated that some columns were linearly dependent. MATLAB automatically selected only the necessary independent components.*

---

### ðŸ”Ž Step 2: Selecting Components (95% Variance)

```matlab
numPCs = find(cumsum(explained) >= 95, 1);
```

ðŸ“Œ **Result**: Only **4 principal components** were needed to preserve over 95% of the total varianceâ€”meaning we successfully reduced the dataset from many dimensions down to just 4 with minimal information loss.

---

## ðŸ“‰ Graphs and What They Show

### ðŸ“Š 1. **Pareto Chart â€“ Variance Explained**

```matlab
pareto(explained);
xlabel('Principal Component');
ylabel('Variance Explained (%)');
title('PCA Pareto Chart');
```

#### âœ… What It Shows:

* Each bar represents how much variance a component captures.
* The line shows **cumulative variance**.
* **Interpretation**: The first few components contribute the most to explaining the dataset. This helps decide how many components to keepâ€”here, 4 was sufficient to cross 95%.

---

### ðŸ” 2. **2D PCA Scatter Plots (PC1 vs PC2, PC1 vs PC3, etc.)**

```matlab
for i = 1:numToPlot
    for j = i+1:numToPlot
        gscatter(scoreTrain(:,i), scoreTrain(:,j), Y_train);
        ...
    end
end
```

#### âœ… What They Show:

* Each point is a sample projected in 2D PCA space.
* Points are colored by class (e.g., Normal, SVEB, VEB).
* **Interpretation**: Helps visually assess how well different heartbeat classes are separated in low-dimensional space. Overlaps suggest harder classification boundaries.

ðŸ“Œ **Useful for** identifying class clusters, overlap, and potential misclassifications.

---

### ðŸ§­ 3. **3D PCA Scatter Plot**

```matlab
scatter3(scoreTrain(idx,1), scoreTrain(idx,2), scoreTrain(idx,3), ...);
```

#### âœ… What It Shows:

* A 3D view using PC1, PC2, and PC3.
* Each point is colored by class.
* **Interpretation**: Gives a richer spatial understanding of how different arrhythmia classes cluster. If classes form distinct blobs in 3D, models will likely perform well.

> Tip: Use `view(135,30)` to rotate the graph for optimal perspective.

---

## âœ… Final Thoughts

By reducing our feature space from dozens of features to just **4 principal components**, we not only sped up the classification model but also created clear visualizations to understand class separability.

---

### ðŸ§  Best Practices

* Always apply PCA only on the training data and **transform** the test/validation data using the same `coeff` and `mu`.
* PCA is most effective when followed by model training like Random Forest, k-NN, or SVM for better performance and reduced overfitting.
